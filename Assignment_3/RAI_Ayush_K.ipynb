{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small data and deep learning\n",
    "This mini-project proposes to study several techniques for improving challenging context, in which few data and resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
    "\n",
    "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   XXX  | XXX | XXX | XXX |\n",
    "\n",
    "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
    "\n",
    "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
    "\n",
    "__The total file sizes should not exceed 2MB. Please name your notebook (LASTNAME)\\_(FIRSTNAME).ipynb, zip/tar it with any necessary files required to run your notebook, in a compressed file named (LASTNAME)\\_(FIRSTNAME).X where X is the corresponding extension. Zip/tar files exceeding 2MB will not be considered for grading. Submit the compressed file via the submission link provided on the website of the class.__\n",
    "\n",
    "You can use https://colab.research.google.com/ to run your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set creation\n",
    "__Question 1:__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "import warnings\n",
    "import visdom\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './cifar10-data'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = data.DataLoader(trainset, batch_size=4, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dog   dog  frog   cat\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_subset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Images:  100\n",
      "Total Test Images:  10000\n"
     ]
    }
   ],
   "source": [
    "print('Total Train Images: ', len(train_subset_loader)*train_subset_loader.batch_size)\n",
    "print('Total Test Images: ', len(testloader)*testloader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing procedure\n",
    "__Question 2:__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the training procedure may be difficult because of the low number of labeled data we are using for the model training. We may have quite a hard time resolving overfitting problems: we have few gold data.\n",
    "\n",
    "As seen in the class, solutions exists:\n",
    "* We can use semi-supervized learning to try to make guesses about unlabeled train data\n",
    "* Use transfer learning, and just fine-tune our model without the need of a huge great dataset\n",
    "* Weak supervision or traditional supervision are also existing solutions, but we will not use them as they involve to ask for a new labelling round, even at higher/abstract level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw approach: the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performances with reported number from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
    "\n",
    "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
    "\n",
    "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context, as those researchers had access to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3:__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1409.1556 ). If possible, please report the accuracy obtained on the whole dataset, as well as the reference paper/GitHub link you might have used.\n",
    "\n",
    "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (~5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adapted from Kuanglio's Implementation : https://github.com/kuangliu/pytorch-cifar/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils : https://github.com/kuangliu/pytorch-cifar/blob/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "term_width = 1\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapted from main https://github.com/kuangliu/pytorch-cifar/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [==========================================================>......]  Step: 487ms | Tot: 4s760ms | Loss: 2.336 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s659ms | Tot: 1m8s | Loss: 2.318 | Acc: 10.000% (1000/10000) 20/20 0 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [==========================================================>......]  Step: 548ms | Tot: 5s248ms | Loss: 2.317 | Acc: 18.000% (18/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s478ms | Tot: 1m7s | Loss: 2.431 | Acc: 10.020% (1002/10000) 20/20   \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [==========================================================>......]  Step: 508ms | Tot: 4s785ms | Loss: 2.356 | Acc: 19.000% (19/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s680ms | Tot: 1m8s | Loss: 2.434 | Acc: 14.170% (1417/10000) 20/20 0  \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [==========================================================>......]  Step: 573ms | Tot: 4s713ms | Loss: 1.843 | Acc: 40.000% (40/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s631ms | Tot: 1m9s | Loss: 2.400 | Acc: 18.980% (1898/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [==========================================================>......]  Step: 597ms | Tot: 5s67ms | Loss: 1.557 | Acc: 45.000% (45/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s504ms | Tot: 1m8s | Loss: 3.326 | Acc: 16.080% (1608/10000) 20/20 20 \n",
      "\n",
      "Epoch: 5\n",
      " [==========================================================>......]  Step: 564ms | Tot: 5s155ms | Loss: 1.369 | Acc: 53.000% (53/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s662ms | Tot: 1m7s | Loss: 3.373 | Acc: 20.080% (2008/10000) 20/20 0  \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [==========================================================>......]  Step: 498ms | Tot: 4s617ms | Loss: 1.203 | Acc: 54.000% (54/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s564ms | Tot: 1m6s | Loss: 4.496 | Acc: 19.290% (1929/10000) 20/20 20 \n",
      "\n",
      "Epoch: 7\n",
      " [==========================================================>......]  Step: 512ms | Tot: 4s921ms | Loss: 1.110 | Acc: 63.000% (63/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s431ms | Tot: 1m6s | Loss: 3.242 | Acc: 17.820% (1782/10000) 20/20  0 \n",
      "\n",
      "Epoch: 8\n",
      " [==========================================================>......]  Step: 594ms | Tot: 4s696ms | Loss: 1.012 | Acc: 69.000% (69/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s494ms | Tot: 1m6s | Loss: 4.103 | Acc: 20.270% (2027/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [==========================================================>......]  Step: 466ms | Tot: 4s886ms | Loss: 0.999 | Acc: 62.000% (62/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s771ms | Tot: 1m7s | Loss: 3.611 | Acc: 19.750% (1975/10000) 20/20 20 \n",
      "\n",
      "Epoch: 10\n",
      " [==========================================================>......]  Step: 581ms | Tot: 5s624ms | Loss: 0.666 | Acc: 79.000% (79/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s731ms | Tot: 1m8s | Loss: 4.240 | Acc: 15.470% (1547/10000) 20/20 20 \n",
      "\n",
      "Epoch: 11\n",
      " [==========================================================>......]  Step: 518ms | Tot: 4s916ms | Loss: 0.641 | Acc: 79.000% (79/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s447ms | Tot: 1m8s | Loss: 5.081 | Acc: 17.580% (1758/10000) 20/20 20 \n",
      "\n",
      "Epoch: 12\n",
      " [==========================================================>......]  Step: 500ms | Tot: 4s717ms | Loss: 0.555 | Acc: 82.000% (82/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s454ms | Tot: 1m6s | Loss: 4.180 | Acc: 20.040% (2004/10000) 20/20  0 \n",
      "\n",
      "Epoch: 13\n",
      " [==========================================================>......]  Step: 500ms | Tot: 4s692ms | Loss: 0.520 | Acc: 82.000% (82/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s454ms | Tot: 1m6s | Loss: 4.236 | Acc: 21.940% (2194/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      " [==========================================================>......]  Step: 522ms | Tot: 4s556ms | Loss: 0.484 | Acc: 85.000% (85/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s457ms | Tot: 1m6s | Loss: 3.742 | Acc: 21.710% (2171/10000) 20/20 20 \n",
      "\n",
      "Epoch: 15\n",
      " [==========================================================>......]  Step: 491ms | Tot: 4s989ms | Loss: 0.338 | Acc: 91.000% (91/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s519ms | Tot: 1m6s | Loss: 3.826 | Acc: 22.200% (2220/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      " [==========================================================>......]  Step: 428ms | Tot: 4s524ms | Loss: 0.220 | Acc: 92.000% (92/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s527ms | Tot: 1m7s | Loss: 3.684 | Acc: 21.520% (2152/10000) 20/20 0  \n",
      "\n",
      "Epoch: 17\n",
      " [==========================================================>......]  Step: 544ms | Tot: 4s986ms | Loss: 0.161 | Acc: 97.000% (97/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s437ms | Tot: 1m6s | Loss: 3.794 | Acc: 21.900% (2190/10000) 20/20 20 \n",
      "\n",
      "Epoch: 18\n",
      " [==========================================================>......]  Step: 430ms | Tot: 4s561ms | Loss: 0.150 | Acc: 96.000% (96/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s707ms | Tot: 1m6s | Loss: 3.877 | Acc: 22.190% (2219/10000) 20/20 20 \n",
      "\n",
      "Epoch: 19\n",
      " [==========================================================>......]  Step: 639ms | Tot: 5s128ms | Loss: 0.126 | Acc: 98.000% (98/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s483ms | Tot: 1m6s | Loss: 3.786 | Acc: 23.190% (2319/10000) 20/20 20 \n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "total_epochs = 20\n",
    "learning_rate=0.01\n",
    "try_cuda=False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data Transformation\n",
    "transform_train = transforms.Compose([\n",
    "#    transforms.RandomCrop(32, padding=4),\n",
    " #   transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=10, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "\n",
    "net = ResNet18()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_subset_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(train_subset_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return (train_loss/(batch_idx+1)), 100.*correct/total\n",
    "        \n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "                \n",
    "    return (test_loss/(batch_idx+1)), 100.*correct/total\n",
    "                            \n",
    "            \n",
    "def train_resnet(total_epochs):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    epoch_list = list(range(total_epochs))\n",
    "            \n",
    "    for epoch in range(start_epoch, start_epoch+total_epochs):\n",
    "        train_loss, train_accuracy = train(epoch)\n",
    "        test_loss, test_accuracy = test(epoch)\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        test_loss_list.append(train_loss)\n",
    "        test_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "    return train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list\n",
    "\n",
    "train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list = train_resnet(total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   ResNet18  | 20 | 98% | 23.190% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-like architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4:__ Same question as before, but with a *VGG*. Which model do you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kuanglio's Implementation : https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [==========================================================>......]  Step: 481ms | Tot: 4s662ms | Loss: 2.502 | Acc: 13.000% (13/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s86ms | Tot: 40s462ms | Loss: 2.306 | Acc: 10.000% (1000/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [==========================================================>......]  Step: 452ms | Tot: 4s315ms | Loss: 2.680 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s280ms | Tot: 40s908ms | Loss: 2.502 | Acc: 10.030% (1003/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [==========================================================>......]  Step: 587ms | Tot: 4s720ms | Loss: 3.079 | Acc: 13.000% (13/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s110ms | Tot: 40s800ms | Loss: 35.348 | Acc: 10.310% (1031/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [==========================================================>......]  Step: 567ms | Tot: 4s849ms | Loss: 2.700 | Acc: 16.000% (16/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s81ms | Tot: 40s508ms | Loss: 57.343 | Acc: 10.870% (1087/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [==========================================================>......]  Step: 596ms | Tot: 4s921ms | Loss: 2.659 | Acc: 11.000% (11/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s305ms | Tot: 44s998ms | Loss: 7.510 | Acc: 11.900% (1190/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [==========================================================>......]  Step: 690ms | Tot: 5s166ms | Loss: 2.388 | Acc: 18.000% (18/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s138ms | Tot: 41s546ms | Loss: 2.362 | Acc: 10.740% (1074/10000) 20/20 \n",
      "\n",
      "Epoch: 6\n",
      " [==========================================================>......]  Step: 458ms | Tot: 4s647ms | Loss: 2.280 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s115ms | Tot: 40s905ms | Loss: 2.490 | Acc: 10.000% (1000/10000) 20/20 \n",
      "\n",
      "Epoch: 7\n",
      " [==========================================================>......]  Step: 550ms | Tot: 4s560ms | Loss: 2.328 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s146ms | Tot: 41s546ms | Loss: 5.748 | Acc: 12.350% (1235/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [==========================================================>......]  Step: 592ms | Tot: 4s927ms | Loss: 2.253 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s249ms | Tot: 43s880ms | Loss: 3.937 | Acc: 12.530% (1253/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [==========================================================>......]  Step: 652ms | Tot: 5s113ms | Loss: 2.265 | Acc: 13.000% (13/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s120ms | Tot: 43s668ms | Loss: 2.561 | Acc: 11.080% (1108/10000) 20/20 \n",
      "\n",
      "Epoch: 10\n",
      " [==========================================================>......]  Step: 548ms | Tot: 4s348ms | Loss: 2.199 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s191ms | Tot: 42s116ms | Loss: 2.744 | Acc: 13.120% (1312/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      " [==========================================================>......]  Step: 537ms | Tot: 4s718ms | Loss: 2.262 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s111ms | Tot: 40s411ms | Loss: 2.396 | Acc: 11.470% (1147/10000) 20/20 \n",
      "\n",
      "Epoch: 12\n",
      " [==========================================================>......]  Step: 471ms | Tot: 4s673ms | Loss: 2.185 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s103ms | Tot: 40s162ms | Loss: 2.375 | Acc: 11.100% (1110/10000) 20/20 \n",
      "\n",
      "Epoch: 13\n",
      " [==========================================================>......]  Step: 536ms | Tot: 4s621ms | Loss: 2.171 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s79ms | Tot: 40s90ms | Loss: 2.363 | Acc: 12.630% (1263/10000) 20/20  \n",
      "\n",
      "Epoch: 14\n",
      " [==========================================================>......]  Step: 469ms | Tot: 4s680ms | Loss: 2.145 | Acc: 20.000% (20/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s83ms | Tot: 40s290ms | Loss: 2.403 | Acc: 11.350% (1135/10000) 20/20 \n",
      "\n",
      "Epoch: 15\n",
      " [==========================================================>......]  Step: 560ms | Tot: 4s637ms | Loss: 2.141 | Acc: 20.000% (20/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s67ms | Tot: 40s608ms | Loss: 2.409 | Acc: 11.290% (1129/10000) 20/20 \n",
      "\n",
      "Epoch: 16\n",
      " [==========================================================>......]  Step: 518ms | Tot: 4s699ms | Loss: 2.221 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s114ms | Tot: 40s162ms | Loss: 2.316 | Acc: 13.560% (1356/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      " [==========================================================>......]  Step: 528ms | Tot: 4s580ms | Loss: 2.108 | Acc: 25.000% (25/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s89ms | Tot: 40s476ms | Loss: 2.367 | Acc: 12.640% (1264/10000) 20/20 \n",
      "\n",
      "Epoch: 18\n",
      " [==========================================================>......]  Step: 536ms | Tot: 4s656ms | Loss: 2.144 | Acc: 22.000% (22/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s95ms | Tot: 40s59ms | Loss: 2.526 | Acc: 15.430% (1543/10000) 20/20  \n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      " [==========================================================>......]  Step: 448ms | Tot: 4s594ms | Loss: 2.019 | Acc: 24.000% (24/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s94ms | Tot: 40s247ms | Loss: 2.396 | Acc: 12.150% (1215/10000) 20/20 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "total_epochs = 20\n",
    "learning_rate=0.01\n",
    "try_cuda=False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data Transformation\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=10, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = VGG('VGG19')\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_subset_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(train_subset_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return (train_loss/(batch_idx+1)), 100.*correct/total\n",
    "        \n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "                \n",
    "    return (test_loss/(batch_idx+1)), 100.*correct/total\n",
    "                            \n",
    "def train_vgg(total_epochs):\n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    epoch_list = list(range(total_epochs))\n",
    "            \n",
    "    for epoch in range(start_epoch, start_epoch+total_epochs):\n",
    "        train_loss, train_accuracy = train(epoch)\n",
    "        test_loss, test_accuracy = test(epoch)\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        test_loss_list.append(train_loss)\n",
    "        test_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "    return train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list\n",
    "\n",
    "train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list = train_vgg(total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   VGG19  | 20 | 24% | 12.15 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained ResNet18 and VGG19 models. And for the suggested training method, ResNet18 is a better choice than VGG19, since it converges faster and achieves a better test accuracy as shown in the two tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "\n",
    "__Question 5:__ Pick a model from the list above, adapt it to CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained Model Used : ResNet50. Finetuning the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 9.3712\n",
      "Train Accuracy:  10.0\n",
      "train Loss: 18.2543\n",
      "Train Accuracy:  15.0\n",
      "train Loss: 27.3396\n",
      "Train Accuracy:  15.0\n",
      "train Loss: 37.4462\n",
      "Train Accuracy:  15.0\n",
      "train Loss: 47.6611\n",
      "Train Accuracy:  12.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 1s877ms | Tot: 36s495ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "train Loss: 7.4992\n",
      "Train Accuracy:  30.0\n",
      "train Loss: 16.8081\n",
      "Train Accuracy:  22.5\n",
      "train Loss: 24.6799\n",
      "Train Accuracy:  28.333333333333332\n",
      "train Loss: 32.1864\n",
      "Train Accuracy:  32.5\n",
      "train Loss: 39.3928\n",
      "Train Accuracy:  34.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 2s81ms | Tot: 39s721ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 2\n",
      "train Loss: 3.6204\n",
      "Train Accuracy:  80.0\n",
      "train Loss: 6.7761\n",
      "Train Accuracy:  80.0\n",
      "train Loss: 10.5067\n",
      "Train Accuracy:  73.33333333333333\n",
      "train Loss: 12.7025\n",
      "Train Accuracy:  76.25\n",
      "train Loss: 15.4337\n",
      "Train Accuracy:  76.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s915ms | Tot: 39s85ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 3\n",
      "train Loss: 1.0916\n",
      "Train Accuracy:  95.0\n",
      "train Loss: 2.0869\n",
      "Train Accuracy:  95.0\n",
      "train Loss: 2.7000\n",
      "Train Accuracy:  96.66666666666667\n",
      "train Loss: 4.1972\n",
      "Train Accuracy:  95.0\n",
      "train Loss: 4.7256\n",
      "Train Accuracy:  96.0\n",
      "Training complete in 0m 40s\n",
      " [=============================================================>...]  Step: 1s997ms | Tot: 38s281ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 4\n",
      "train Loss: 0.5772\n",
      "Train Accuracy:  95.0\n",
      "train Loss: 0.7474\n",
      "Train Accuracy:  97.5\n",
      "train Loss: 0.9106\n",
      "Train Accuracy:  98.33333333333333\n",
      "train Loss: 0.9908\n",
      "Train Accuracy:  98.75\n",
      "train Loss: 1.2614\n",
      "Train Accuracy:  99.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 2s78ms | Tot: 38s800ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 5\n",
      "train Loss: 0.1004\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1253\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1527\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.4731\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.7935\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 2s1ms | Tot: 37s460ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20  \n",
      "\n",
      "Epoch: 6\n",
      "train Loss: 0.0973\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1393\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1667\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1988\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.3580\n",
      "Train Accuracy:  99.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s928ms | Tot: 37s712ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 7\n",
      "train Loss: 0.0535\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.4529\n",
      "Train Accuracy:  97.5\n",
      "train Loss: 0.4622\n",
      "Train Accuracy:  98.33333333333333\n",
      "train Loss: 0.5865\n",
      "Train Accuracy:  98.75\n",
      "train Loss: 0.6468\n",
      "Train Accuracy:  99.0\n",
      "Training complete in 0m 42s\n",
      " [=============================================================>...]  Step: 1s918ms | Tot: 38s410ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 8\n",
      "train Loss: 0.0686\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1427\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2103\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2384\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2661\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s934ms | Tot: 39s345ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 9\n",
      "train Loss: 0.0480\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1636\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1748\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2180\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2373\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 2s142ms | Tot: 38s491ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 10\n",
      "train Loss: 0.0040\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0467\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0774\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0820\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1712\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 1s997ms | Tot: 38s215ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 11\n",
      "train Loss: 0.0170\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0383\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0484\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0916\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0967\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 1s917ms | Tot: 38s807ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 12\n",
      "train Loss: 0.0166\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1172\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2034\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2185\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2497\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 1s883ms | Tot: 36s483ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 13\n",
      "train Loss: 0.1300\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1676\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1814\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1890\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1973\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 38s\n",
      " [=============================================================>...]  Step: 1s988ms | Tot: 37s633ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 14\n",
      "train Loss: 0.0086\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0201\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0261\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0346\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0412\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 2s16ms | Tot: 39s120ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 15\n",
      "train Loss: 0.1727\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1823\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2025\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2474\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2534\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s948ms | Tot: 38s336ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 16\n",
      "train Loss: 0.0071\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0165\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0310\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0395\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.1007\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s916ms | Tot: 38s559ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 17\n",
      "train Loss: 0.0066\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0113\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0131\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.2215\n",
      "Train Accuracy:  98.75\n",
      "train Loss: 0.2694\n",
      "Train Accuracy:  99.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s921ms | Tot: 38s356ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 18\n",
      "train Loss: 0.0085\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0165\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0213\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0292\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0326\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s959ms | Tot: 36s970ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n",
      "\n",
      "Epoch: 19\n",
      "train Loss: 0.0060\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0094\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0161\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0417\n",
      "Train Accuracy:  100.0\n",
      "train Loss: 0.0636\n",
      "Train Accuracy:  100.0\n",
      "Training complete in 0m 39s\n",
      " [=============================================================>...]  Step: 1s980ms | Tot: 38s651ms | Loss: 2.392 | Test Acc: 11.880% (1188/10000) 20/20 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "DATA_PATH = './cifar10-data'\n",
    "input_shape = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Data Transformation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(input_shape),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "total_epochs=20\n",
    "lr=0.01\n",
    "try_cuda=False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=20, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "## Load the model based on ResNet50\n",
    "resnet_based = torchvision.models.resnet50(pretrained='imagenet')\n",
    "num_ftrs = resnet_based.fc.in_features\n",
    "resnet_based.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "resnet_based = resnet_based.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "print(resnet_based)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(resnet_based.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "def train_model(model, criterion, optimizer,epoch):\n",
    "    since = time.time()\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    #set model to trainable\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for i, data in enumerate(train_subset_loader):\n",
    "        inputs , labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "          \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs  = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        print('{} Loss: {:.4f}'.format('train', train_loss / len(train_subset_loader)))\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        print('Train Accuracy: ', (100*correct)/total)\n",
    "          \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "def test_model(model,epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "def main(total_epochs):\n",
    "    for epoch in range(start_epoch, start_epoch+total_epochs):\n",
    "        train_model(resnet_based, criterion, optimizer_ft,epoch)\n",
    "        test_model(resnet_based,epoch)\n",
    "        \n",
    "main(total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   ResNet50-Pretrained  | 5 | 100% | 11.88% |\n",
    "\n",
    "Although I trained the model for 20 epochs but training accuracy reached 100% on the 5th Epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs correspond to an unsupervised technique for generating images. In https://arxiv.org/pdf/1511.06434.pdf, Sec. 5.1 shows that the representation obtained from the Discriminator has some nice generalization properties on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6:__  Using for instance a pretrained model from https://github.com/soumith/dcgan.torch combined with https://github.com/pytorch/examples/tree/master/dcgan, propose a model to train on $\\mathcal{X}_{\\text{train}}$. Train it and report its accuracy.\n",
    "\n",
    "*Hint:* You can use the library: https://github.com/bshillingford/python-torchfile to load the weights of a model from torch(Lua) to pytorch(python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Downloaded DCGAN Pretrained Model on CIFAR100 : https://github.com/csinva/pytorch_gan_pretrained/tree/master/cifar100_dcgan_grayscale/weights\n",
    "\n",
    "###### Code Adapted from : https://github.com/pytorch/examples/blob/master/dcgan/main.py \n",
    "\n",
    "##### Note : We only need the discriminator (and not the generator) of DCGAN for feature extraction on CIFAR10 Dataset. And then SVM based supervised model is used for classification of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from sklearn import svm\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batchSize=20  #128 #input batch size\n",
    "imageSize= 32  #the height / width of the input image to network\n",
    "nz=256  #size of the latent z vector\n",
    "ngf=128  # Starting Number of Filters in Generator\n",
    "ndf=128  # Starting Number of Filters in Discriminator\n",
    "nc = 1  # Number of Channels\n",
    "\n",
    "cudnn.benchmark = True\n",
    "discriminator_weight_path = './netD_epoch_299.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator Structure of the DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc=1, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 2, 2, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(0))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "    def get_features(self, input):\n",
    "        \n",
    "        out1 = self.main[0](input)\n",
    "        out_conv1 = self.main[1](out1)\n",
    "        \n",
    "        out2 = self.main[2](out_conv1)\n",
    "        out3 = self.main[3](out2)\n",
    "        out_conv2 = self.main[4](out3)\n",
    "        \n",
    "        \n",
    "        out4 = self.main[5](out_conv2)\n",
    "        out5 = self.main[6](out4)\n",
    "        out_conv3 = self.main[7](out5)\n",
    "        \n",
    "        out6 = self.main[8](out_conv3)\n",
    "        out7 = self.main[9](out6)\n",
    "        out_conv4 = self.main[10](out7)\n",
    "        \n",
    "        out8 = self.main[11](out_conv4)\n",
    "        out_conv5 = self.main[12](out8)\n",
    "\n",
    "        max_pool1 = nn.MaxPool2d(int(out_conv1.size(2) / 4))\n",
    "        max_pool2 = nn.MaxPool2d(int(out_conv2.size(2) / 4))\n",
    "        max_pool3 = nn.MaxPool2d(int(out_conv3.size(2) / 4))\n",
    "        # max_pool4 = nn.MaxPool2d(int(out_conv4.size(2) / 4))\n",
    "\n",
    "        vector1 = max_pool1(out_conv1).view(input.size(0), -1).squeeze(1)\n",
    "        vector2 = max_pool2(out_conv2).view(input.size(0), -1).squeeze(1)\n",
    "        vector3 = max_pool3(out_conv3).view(input.size(0), -1).squeeze(1)\n",
    "        # vector4 = max_pool4(out_conv4).view(input.size(0), -1).squeeze(1)\n",
    "\n",
    "        return torch.cat((vector1, vector2, vector3), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction using Discriminator of DCGAN + SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "main.0.weight torch.Size([64, 1, 4, 4])\n",
      "main.2.weight torch.Size([128, 64, 4, 4])\n",
      "main.3.weight torch.Size([128])\n",
      "main.3.bias torch.Size([128])\n",
      "main.5.weight torch.Size([256, 128, 4, 4])\n",
      "main.6.weight torch.Size([256])\n",
      "main.6.bias torch.Size([256])\n",
      "main.8.weight torch.Size([512, 256, 4, 4])\n",
      "main.9.weight torch.Size([512])\n",
      "main.9.bias torch.Size([512])\n",
      "main.11.weight torch.Size([1, 512, 2, 2])\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "train_feature shape (100, 7168)\n",
      "train_labels shape torch.Size([100])\n",
      "test_feature shape (10000, 7168)\n",
      "test_labels shape torch.Size([10000])\n",
      "train svm\n",
      "predict svm\n",
      "svm results: 0.1009 Test Accuracy\n",
      "svm results: 0.4100 Train accuracy\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.CenterCrop(imageSize),\n",
    "                               transforms.Grayscale(),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=100, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#Initialize a Discriminator\n",
    "netD = Discriminator()\n",
    "\n",
    "for name, param in netD.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.shape)\n",
    "\n",
    "netD.load_state_dict(torch.load(discriminator_weight_path))\n",
    "print(netD)\n",
    "\n",
    "netD.cuda()\n",
    "\n",
    "netD.eval()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize).cuda()\n",
    "train_features = np.array([])\n",
    "train_labels = np.array([])\n",
    "\n",
    "test_features = np.array([])\n",
    "test_labels = np.array([])\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "#Get training Set Features\n",
    "for i, data in enumerate(train_subset_loader):\n",
    "    imgs, label = data\n",
    "    imgs = imgs.cuda()\n",
    "    input.resize_as_(imgs).copy_(imgs)\n",
    "    input_v = Variable(input)\n",
    "\n",
    "    feature = netD.get_features(input_v)\n",
    "    feature = feature.data.cpu().numpy()\n",
    "    feature = feature.astype(np.float16)\n",
    "        \n",
    "    if train_features.size == 0:\n",
    "        train_features = feature\n",
    "        train_labels = label\n",
    "    else:\n",
    "        train_features = np.concatenate((train_features, feature), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, label), axis=0)\n",
    "        \n",
    "#Get Validation Set Features\n",
    "for i, data in enumerate(testloader):\n",
    "    imgs, label = data\n",
    "    imgs = imgs.cuda()\n",
    "    input.resize_as_(imgs).copy_(imgs)\n",
    "    input_v = Variable(input)\n",
    "\n",
    "    feature = netD.get_features(input_v)\n",
    "    feature = feature.data.cpu().numpy()\n",
    "    feature = feature.astype(np.float16)\n",
    "        \n",
    "    if test_features.size == 0:\n",
    "        test_features = feature\n",
    "        test_labels = label\n",
    "    else:\n",
    "        test_features = np.concatenate((test_features, feature), axis=0)\n",
    "        test_labels = np.concatenate((test_labels, label), axis=0)\n",
    "        \n",
    "#-----------------------------------------------\n",
    "\n",
    "print('train_feature shape', train_features.shape)\n",
    "print('train_labels shape', train_labels.shape)\n",
    "\n",
    "print('test_feature shape', test_features.shape)\n",
    "print('test_labels shape', test_labels.shape)\n",
    "\n",
    "print('train svm')\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(train_features, train_labels)\n",
    "    \n",
    "print('predict svm')\n",
    "test_labels = test_labels.squeeze()\n",
    "predicted_test_labels = clf.predict(test_features)\n",
    "predicted_train_labels = clf.predict(train_features)\n",
    "\n",
    "test_labels = test_labels.data.numpy()\n",
    "train_labels = train_labels.data.numpy()\n",
    "\n",
    "# Compute Test Accuracy\n",
    "a = predicted_test_labels == test_labels\n",
    "test_accuracy = np.sum(predicted_test_labels == test_labels) / len(test_labels)\n",
    "print('svm results: %.4f Test Accuracy'%test_accuracy )\n",
    "\n",
    "# Compute Train Accuracy\n",
    "a = predicted_train_labels == train_labels\n",
    "train_accuracy = np.sum(predicted_train_labels == train_labels) / len(train_labels)\n",
    "print('svm results: %.4f Train accuracy'%train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction from DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   Pretrained-DCGAN+SVM  | 290 (Pretrained DCGAN) | 41% | 10.09% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating *a priori*\n",
    "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
    "\n",
    "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
    "\n",
    "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
    "\n",
    "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
    "\n",
    "Otherwise, one has to handle several boundary effects.\n",
    "\n",
    "__Question 7:__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data augmentation on extremely small images of size 32x32 can cause many defects.\n",
    "\n",
    "* Aliasing is most likely to appear in the transformed data. However this could be partly mitigated with an anti aliasing filter;\n",
    "\n",
    "* When we come to scaling, translation or non-square rotation, we will face the big issue of lack of pixel. When performing a non-square rotation,  we will have to zoom into the image in order not to have dark spots on the corners, but with that few pexels, we may reconstruct non recognizable images. Zooming is not everything, we will have to resize the image into 32*32 dimension, using interpolation. We will have to duplicate pixels and facing an even bigger pixelisation. To adress such problem, we can apply gaussian filters to our images in order to smooth the low resolution. We could also try to refine the image quality using GANs\n",
    "\n",
    "* Scaling effects reduces the resolution of the image and this could be extremely harmful. To get rid of this, we could use sub-pixel methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8:__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ and __Question 4__ with them and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Question 3 (with Data Augmentation) : ResNet with Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [==========================================================>......]  Step: 482ms | Tot: 4s666ms | Loss: 2.440 | Acc: 8.000% (8/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s380ms | Tot: 1m5s | Loss: 2.324 | Acc: 10.000% (1000/10000) 20/20 0 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [==========================================================>......]  Step: 562ms | Tot: 4s852ms | Loss: 2.500 | Acc: 13.000% (13/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s476ms | Tot: 1m5s | Loss: 2.374 | Acc: 10.690% (1069/10000) 20/20 0 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [==========================================================>......]  Step: 565ms | Tot: 5s406ms | Loss: 2.309 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s465ms | Tot: 1m6s | Loss: 3.735 | Acc: 10.890% (1089/10000) 20/20 0 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [==========================================================>......]  Step: 560ms | Tot: 5s154ms | Loss: 2.481 | Acc: 13.000% (13/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s384ms | Tot: 1m5s | Loss: 19.863 | Acc: 10.410% (1041/10000) 20/20 0 \n",
      "\n",
      "Epoch: 4\n",
      " [==========================================================>......]  Step: 601ms | Tot: 5s9ms | Loss: 2.293 | Acc: 25.000% (25/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s398ms | Tot: 1m6s | Loss: 9.951 | Acc: 11.780% (1178/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [==========================================================>......]  Step: 574ms | Tot: 5s14ms | Loss: 2.087 | Acc: 25.000% (25/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s593ms | Tot: 1m6s | Loss: 9.638 | Acc: 16.580% (1658/10000) 20/20 0  \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [==========================================================>......]  Step: 494ms | Tot: 4s741ms | Loss: 2.141 | Acc: 26.000% (26/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s355ms | Tot: 1m5s | Loss: 7.889 | Acc: 14.610% (1461/10000) 20/20 20 \n",
      "\n",
      "Epoch: 7\n",
      " [==========================================================>......]  Step: 474ms | Tot: 4s991ms | Loss: 2.139 | Acc: 24.000% (24/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s741ms | Tot: 1m7s | Loss: 5.744 | Acc: 14.470% (1447/10000) 20/20  0 \n",
      "\n",
      "Epoch: 8\n",
      " [==========================================================>......]  Step: 631ms | Tot: 5s296ms | Loss: 2.021 | Acc: 33.000% (33/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s379ms | Tot: 1m5s | Loss: 4.310 | Acc: 17.290% (1729/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [==========================================================>......]  Step: 500ms | Tot: 4s532ms | Loss: 1.724 | Acc: 42.000% (42/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s389ms | Tot: 1m5s | Loss: 7.840 | Acc: 14.530% (1453/10000) 20/20 20 \n",
      "\n",
      "Epoch: 10\n",
      " [==========================================================>......]  Step: 498ms | Tot: 5s23ms | Loss: 1.702 | Acc: 37.000% (37/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s408ms | Tot: 1m5s | Loss: 7.621 | Acc: 14.950% (1495/10000) 20/20 20 \n",
      "\n",
      "Epoch: 11\n",
      " [==========================================================>......]  Step: 596ms | Tot: 4s868ms | Loss: 1.630 | Acc: 44.000% (44/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s383ms | Tot: 1m6s | Loss: 6.910 | Acc: 15.320% (1532/10000) 20/20 20 \n",
      "\n",
      "Epoch: 12\n",
      " [==========================================================>......]  Step: 495ms | Tot: 4s594ms | Loss: 1.865 | Acc: 32.000% (32/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s410ms | Tot: 1m4s | Loss: 7.050 | Acc: 14.850% (1485/10000) 20/20 20 \n",
      "\n",
      "Epoch: 13\n",
      " [==========================================================>......]  Step: 573ms | Tot: 4s892ms | Loss: 1.731 | Acc: 40.000% (40/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s399ms | Tot: 1m6s | Loss: 8.668 | Acc: 15.170% (1517/10000) 20/20 20 \n",
      "\n",
      "Epoch: 14\n",
      " [==========================================================>......]  Step: 525ms | Tot: 4s807ms | Loss: 1.792 | Acc: 38.000% (38/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s573ms | Tot: 1m5s | Loss: 11.838 | Acc: 13.600% (1360/10000) 20/20 20 \n",
      "\n",
      "Epoch: 15\n",
      " [==========================================================>......]  Step: 475ms | Tot: 4s621ms | Loss: 1.772 | Acc: 45.000% (45/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s455ms | Tot: 1m5s | Loss: 6.641 | Acc: 13.840% (1384/10000) 20/20 20 \n",
      "\n",
      "Epoch: 16\n",
      " [==========================================================>......]  Step: 544ms | Tot: 4s625ms | Loss: 1.662 | Acc: 46.000% (46/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s403ms | Tot: 1m5s | Loss: 3.495 | Acc: 18.870% (1887/10000) 20/20 20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      " [==========================================================>......]  Step: 527ms | Tot: 4s928ms | Loss: 1.520 | Acc: 48.000% (48/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s328ms | Tot: 1m5s | Loss: 8.742 | Acc: 13.310% (1331/10000) 20/20 20 \n",
      "\n",
      "Epoch: 18\n",
      " [==========================================================>......]  Step: 514ms | Tot: 4s545ms | Loss: 1.655 | Acc: 42.000% (42/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s449ms | Tot: 1m5s | Loss: 7.017 | Acc: 15.660% (1566/10000) 20/20 20 \n",
      "\n",
      "Epoch: 19\n",
      " [==========================================================>......]  Step: 604ms | Tot: 5s47ms | Loss: 1.712 | Acc: 44.000% (44/100) 10/10 \n",
      " [=============================================================>...]  Step: 3s411ms | Tot: 1m6s | Loss: 7.013 | Acc: 12.620% (1262/10000) 20/20 20 \n"
     ]
    }
   ],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1,3,32,32))\n",
    "    print(y.size())\n",
    "\n",
    "    \n",
    "#-------------------------Train--------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "total_epochs = 20\n",
    "learning_rate=0.01\n",
    "try_cuda=False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data Transformation\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=10, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "\n",
    "net = ResNet18()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_subset_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(train_subset_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return (train_loss/(batch_idx+1)), 100.*correct/total\n",
    "        \n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "                \n",
    "    return (test_loss/(batch_idx+1)), 100.*correct/total\n",
    "                            \n",
    "            \n",
    "def train_resnet(total_epochs):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    epoch_list = list(range(total_epochs))\n",
    "            \n",
    "    for epoch in range(start_epoch, start_epoch+total_epochs):\n",
    "        train_loss, train_accuracy = train(epoch)\n",
    "        test_loss, test_accuracy = test(epoch)\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        test_loss_list.append(train_loss)\n",
    "        test_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "    return train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list\n",
    "            \n",
    "train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list = train_resnet(total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   ResNet18 (with Data Augmentation)  | 20 | 44% | 12.6% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4 (with Data Augmentation) : VGG with Tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      " [==========================================================>......]  Step: 433ms | Tot: 4s434ms | Loss: 2.554 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 1s997ms | Tot: 38s805ms | Loss: 2.287 | Acc: 10.010% (1001/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [==========================================================>......]  Step: 527ms | Tot: 4s598ms | Loss: 2.846 | Acc: 9.000% (9/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s178ms | Tot: 40s919ms | Loss: 33.533 | Acc: 10.960% (1096/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [==========================================================>......]  Step: 513ms | Tot: 4s510ms | Loss: 2.613 | Acc: 9.000% (9/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s30ms | Tot: 40s971ms | Loss: 60.702 | Acc: 12.320% (1232/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [==========================================================>......]  Step: 620ms | Tot: 5s152ms | Loss: 3.344 | Acc: 16.000% (16/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s78ms | Tot: 40s582ms | Loss: 123.376 | Acc: 10.000% (1000/10000) 20/20 \n",
      "\n",
      "Epoch: 4\n",
      " [==========================================================>......]  Step: 554ms | Tot: 4s690ms | Loss: 2.794 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s50ms | Tot: 40s107ms | Loss: 146.904 | Acc: 10.090% (1009/10000) 20/20 \n",
      "\n",
      "Epoch: 5\n",
      " [==========================================================>......]  Step: 463ms | Tot: 4s699ms | Loss: 2.382 | Acc: 12.000% (12/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s79ms | Tot: 40s291ms | Loss: 95.778 | Acc: 9.750% (975/10000) 20/20 \n",
      "\n",
      "Epoch: 6\n",
      " [==========================================================>......]  Step: 452ms | Tot: 4s523ms | Loss: 2.429 | Acc: 12.000% (12/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s38ms | Tot: 39s451ms | Loss: 35.522 | Acc: 9.240% (924/10000) 20/20 \n",
      "\n",
      "Epoch: 7\n",
      " [==========================================================>......]  Step: 482ms | Tot: 4s798ms | Loss: 2.360 | Acc: 8.000% (8/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s74ms | Tot: 40s126ms | Loss: 12.042 | Acc: 10.070% (1007/10000) 20/20 \n",
      "\n",
      "Epoch: 8\n",
      " [==========================================================>......]  Step: 609ms | Tot: 4s900ms | Loss: 2.304 | Acc: 14.000% (14/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s66ms | Tot: 40s578ms | Loss: 12.461 | Acc: 9.990% (999/10000) 20/20  \n",
      "\n",
      "Epoch: 9\n",
      " [==========================================================>......]  Step: 567ms | Tot: 4s807ms | Loss: 2.346 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s69ms | Tot: 40s137ms | Loss: 9.898 | Acc: 11.190% (1119/10000) 20/20 \n",
      "\n",
      "Epoch: 10\n",
      " [==========================================================>......]  Step: 501ms | Tot: 4s641ms | Loss: 2.217 | Acc: 17.000% (17/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s74ms | Tot: 39s317ms | Loss: 10.843 | Acc: 11.550% (1155/10000) 20/20 \n",
      "\n",
      "Epoch: 11\n",
      " [==========================================================>......]  Step: 453ms | Tot: 4s566ms | Loss: 2.253 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s57ms | Tot: 39s504ms | Loss: 6.603 | Acc: 10.480% (1048/10000) 20/20 \n",
      "\n",
      "Epoch: 12\n",
      " [==========================================================>......]  Step: 500ms | Tot: 4s485ms | Loss: 2.216 | Acc: 12.000% (12/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s58ms | Tot: 39s593ms | Loss: 8.066 | Acc: 12.670% (1267/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [==========================================================>......]  Step: 560ms | Tot: 4s768ms | Loss: 2.240 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s78ms | Tot: 40s114ms | Loss: 7.166 | Acc: 11.540% (1154/10000) 20/20 \n",
      "\n",
      "Epoch: 14\n",
      " [==========================================================>......]  Step: 485ms | Tot: 4s690ms | Loss: 2.179 | Acc: 20.000% (20/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s40ms | Tot: 39s453ms | Loss: 7.897 | Acc: 12.040% (1204/10000) 20/20 \n",
      "\n",
      "Epoch: 15\n",
      " [==========================================================>......]  Step: 477ms | Tot: 4s638ms | Loss: 2.189 | Acc: 18.000% (18/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s57ms | Tot: 39s445ms | Loss: 6.393 | Acc: 12.350% (1235/10000) 20/20 \n",
      "\n",
      "Epoch: 16\n",
      " [==========================================================>......]  Step: 636ms | Tot: 5s41ms | Loss: 2.224 | Acc: 11.000% (11/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s46ms | Tot: 38s736ms | Loss: 3.798 | Acc: 13.180% (1318/10000) 20/20 \n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      " [==========================================================>......]  Step: 560ms | Tot: 4s409ms | Loss: 2.170 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s55ms | Tot: 39s523ms | Loss: 6.019 | Acc: 10.950% (1095/10000) 20/20 \n",
      "\n",
      "Epoch: 18\n",
      " [==========================================================>......]  Step: 452ms | Tot: 4s323ms | Loss: 2.187 | Acc: 15.000% (15/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s45ms | Tot: 39s353ms | Loss: 3.223 | Acc: 12.940% (1294/10000) 20/20 \n",
      "\n",
      "Epoch: 19\n",
      " [==========================================================>......]  Step: 524ms | Tot: 4s478ms | Loss: 2.210 | Acc: 20.000% (20/100) 10/10 \n",
      " [=============================================================>...]  Step: 2s47ms | Tot: 39s253ms | Loss: 4.992 | Acc: 12.610% (1261/10000) 20/20 \n"
     ]
    }
   ],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "#-------------------------TRAIN--------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "total_epochs = 20\n",
    "learning_rate=0.01\n",
    "try_cuda=False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data Transformation\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "#Define Valid Indices\n",
    "subset_indices = list(range(100))\n",
    "\n",
    "#Using Torch Subset DataLoader to Load the valid indices\n",
    "train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=10, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = VGG('VGG19')\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_subset_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(train_subset_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return (train_loss/(batch_idx+1)), 100.*correct/total\n",
    "        \n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            \n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc\n",
    "                \n",
    "    return (test_loss/(batch_idx+1)), 100.*correct/total\n",
    "                            \n",
    "def train_vgg(total_epochs):\n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_loss_list = []\n",
    "    test_accuracy_list = []\n",
    "    epoch_list = list(range(total_epochs))\n",
    "            \n",
    "    for epoch in range(start_epoch, start_epoch+total_epochs):\n",
    "        train_loss, train_accuracy = train(epoch)\n",
    "        test_loss, test_accuracy = test(epoch)\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "        test_loss_list.append(train_loss)\n",
    "        test_accuracy_list.append(train_accuracy)\n",
    "            \n",
    "    return train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list\n",
    "        \n",
    "train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list, epoch_list = train_vgg(total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   VGG19 (with data augmentation)  | 20 | 20% | 12.61% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets\n",
    "\n",
    "__Question 9:__ Use a Scattering Transform as an input to a ResNet-like architecture. You can find a baseline here: https://arxiv.org/pdf/1703.08961.pdf.\n",
    "\n",
    "*Hint:* You can use the following package: https://www.kymat.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Source : https://www.kymat.io/gallery_2d/cifar_resnet.html#sphx-glr-gallery-2d-cifar-resnet-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Total Train Images:  100\n",
      "Total Test Images:  10000\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.214156\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 1 [10/50000 (10%)]\tLoss: 5.080067\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 1 [20/50000 (20%)]\tLoss: 4.368468\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 1 [30/50000 (30%)]\tLoss: 3.858758\n",
      "Train Accuracy:  12.5\n",
      "Train Epoch: 1 [40/50000 (40%)]\tLoss: 11.150679\n",
      "Train Accuracy:  14.0\n",
      "Train Epoch: 1 [50/50000 (50%)]\tLoss: 11.291357\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 1 [60/50000 (60%)]\tLoss: 11.093459\n",
      "Train Accuracy:  14.285714285714286\n",
      "Train Epoch: 1 [70/50000 (70%)]\tLoss: 7.149952\n",
      "Train Accuracy:  12.5\n",
      "Train Epoch: 1 [80/50000 (80%)]\tLoss: 3.564714\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 1 [90/50000 (90%)]\tLoss: 3.822342\n",
      "Train Accuracy:  13.0\n",
      "\n",
      "Test set: Average loss: 275.0612, Accuracy: 1341/10000 (13.41%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 3.231939\n",
      "Train Accuracy:  0.0\n",
      "Train Epoch: 2 [10/50000 (10%)]\tLoss: 2.706958\n",
      "Train Accuracy:  0.0\n",
      "Train Epoch: 2 [20/50000 (20%)]\tLoss: 3.181203\n",
      "Train Accuracy:  0.0\n",
      "Train Epoch: 2 [30/50000 (30%)]\tLoss: 2.034333\n",
      "Train Accuracy:  5.0\n",
      "Train Epoch: 2 [40/50000 (40%)]\tLoss: 3.177339\n",
      "Train Accuracy:  6.0\n",
      "Train Epoch: 2 [50/50000 (50%)]\tLoss: 3.625654\n",
      "Train Accuracy:  6.666666666666667\n",
      "Train Epoch: 2 [60/50000 (60%)]\tLoss: 2.245128\n",
      "Train Accuracy:  8.571428571428571\n",
      "Train Epoch: 2 [70/50000 (70%)]\tLoss: 2.822674\n",
      "Train Accuracy:  8.75\n",
      "Train Epoch: 2 [80/50000 (80%)]\tLoss: 2.656634\n",
      "Train Accuracy:  7.777777777777778\n",
      "Train Epoch: 2 [90/50000 (90%)]\tLoss: 2.222751\n",
      "Train Accuracy:  7.0\n",
      "\n",
      "Test set: Average loss: 2.7994, Accuracy: 1035/10000 (10.35%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.122603\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 3 [10/50000 (10%)]\tLoss: 2.163000\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 3 [20/50000 (20%)]\tLoss: 2.387457\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 3 [30/50000 (30%)]\tLoss: 2.430145\n",
      "Train Accuracy:  12.5\n",
      "Train Epoch: 3 [40/50000 (40%)]\tLoss: 2.168254\n",
      "Train Accuracy:  16.0\n",
      "Train Epoch: 3 [50/50000 (50%)]\tLoss: 2.482217\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 3 [60/50000 (60%)]\tLoss: 2.262902\n",
      "Train Accuracy:  14.285714285714286\n",
      "Train Epoch: 3 [70/50000 (70%)]\tLoss: 2.390295\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 3 [80/50000 (80%)]\tLoss: 2.078262\n",
      "Train Accuracy:  15.555555555555555\n",
      "Train Epoch: 3 [90/50000 (90%)]\tLoss: 2.326380\n",
      "Train Accuracy:  16.0\n",
      "\n",
      "Test set: Average loss: 2.3657, Accuracy: 1021/10000 (10.21%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.200737\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 4 [10/50000 (10%)]\tLoss: 2.482632\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 4 [20/50000 (20%)]\tLoss: 2.378939\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 4 [30/50000 (30%)]\tLoss: 2.215079\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 4 [40/50000 (40%)]\tLoss: 2.072332\n",
      "Train Accuracy:  14.0\n",
      "Train Epoch: 4 [50/50000 (50%)]\tLoss: 2.100696\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 4 [60/50000 (60%)]\tLoss: 2.140646\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 4 [70/50000 (70%)]\tLoss: 2.257496\n",
      "Train Accuracy:  18.75\n",
      "Train Epoch: 4 [80/50000 (80%)]\tLoss: 2.231370\n",
      "Train Accuracy:  18.88888888888889\n",
      "Train Epoch: 4 [90/50000 (90%)]\tLoss: 2.439886\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.3888, Accuracy: 1024/10000 (10.24%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 2.372836\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 5 [10/50000 (10%)]\tLoss: 2.245321\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 5 [20/50000 (20%)]\tLoss: 2.163270\n",
      "Train Accuracy:  23.333333333333332\n",
      "Train Epoch: 5 [30/50000 (30%)]\tLoss: 2.392380\n",
      "Train Accuracy:  22.5\n",
      "Train Epoch: 5 [40/50000 (40%)]\tLoss: 2.164814\n",
      "Train Accuracy:  22.0\n",
      "Train Epoch: 5 [50/50000 (50%)]\tLoss: 2.246605\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 5 [60/50000 (60%)]\tLoss: 2.328359\n",
      "Train Accuracy:  17.142857142857142\n",
      "Train Epoch: 5 [70/50000 (70%)]\tLoss: 2.065579\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 5 [80/50000 (80%)]\tLoss: 2.173231\n",
      "Train Accuracy:  17.77777777777778\n",
      "Train Epoch: 5 [90/50000 (90%)]\tLoss: 2.099383\n",
      "Train Accuracy:  18.0\n",
      "\n",
      "Test set: Average loss: 2.3827, Accuracy: 1059/10000 (10.59%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 2.079182\n",
      "Train Accuracy:  40.0\n",
      "Train Epoch: 6 [10/50000 (10%)]\tLoss: 2.129506\n",
      "Train Accuracy:  35.0\n",
      "Train Epoch: 6 [20/50000 (20%)]\tLoss: 2.214716\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 6 [30/50000 (30%)]\tLoss: 2.130608\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 6 [40/50000 (40%)]\tLoss: 2.232804\n",
      "Train Accuracy:  24.0\n",
      "Train Epoch: 6 [50/50000 (50%)]\tLoss: 2.510484\n",
      "Train Accuracy:  23.333333333333332\n",
      "Train Epoch: 6 [60/50000 (60%)]\tLoss: 2.186757\n",
      "Train Accuracy:  24.285714285714285\n",
      "Train Epoch: 6 [70/50000 (70%)]\tLoss: 2.109950\n",
      "Train Accuracy:  22.5\n",
      "Train Epoch: 6 [80/50000 (80%)]\tLoss: 2.346984\n",
      "Train Accuracy:  21.11111111111111\n",
      "Train Epoch: 6 [90/50000 (90%)]\tLoss: 2.404150\n",
      "Train Accuracy:  20.0\n",
      "\n",
      "Test set: Average loss: 2.4303, Accuracy: 1119/10000 (11.19%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.998206\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 7 [10/50000 (10%)]\tLoss: 2.085276\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 7 [20/50000 (20%)]\tLoss: 2.459328\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 7 [30/50000 (30%)]\tLoss: 2.203686\n",
      "Train Accuracy:  17.5\n",
      "Train Epoch: 7 [40/50000 (40%)]\tLoss: 2.173557\n",
      "Train Accuracy:  14.0\n",
      "Train Epoch: 7 [50/50000 (50%)]\tLoss: 2.434156\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 7 [60/50000 (60%)]\tLoss: 2.153979\n",
      "Train Accuracy:  15.714285714285714\n",
      "Train Epoch: 7 [70/50000 (70%)]\tLoss: 2.045266\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 7 [80/50000 (80%)]\tLoss: 2.131511\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 7 [90/50000 (90%)]\tLoss: 2.232137\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.4055, Accuracy: 1144/10000 (11.44%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 2.341034\n",
      "Train Accuracy:  0.0\n",
      "Train Epoch: 8 [10/50000 (10%)]\tLoss: 1.968695\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 8 [20/50000 (20%)]\tLoss: 1.995555\n",
      "Train Accuracy:  23.333333333333332\n",
      "Train Epoch: 8 [30/50000 (30%)]\tLoss: 2.313899\n",
      "Train Accuracy:  17.5\n",
      "Train Epoch: 8 [40/50000 (40%)]\tLoss: 2.094371\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 8 [50/50000 (50%)]\tLoss: 2.192897\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 8 [60/50000 (60%)]\tLoss: 2.149194\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 8 [70/50000 (70%)]\tLoss: 2.530156\n",
      "Train Accuracy:  17.5\n",
      "Train Epoch: 8 [80/50000 (80%)]\tLoss: 2.072636\n",
      "Train Accuracy:  18.88888888888889\n",
      "Train Epoch: 8 [90/50000 (90%)]\tLoss: 2.368178\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.4244, Accuracy: 1065/10000 (10.65%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 2.531330\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 9 [10/50000 (10%)]\tLoss: 2.012445\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 9 [20/50000 (20%)]\tLoss: 2.368998\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 9 [30/50000 (30%)]\tLoss: 2.351455\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 9 [40/50000 (40%)]\tLoss: 2.093143\n",
      "Train Accuracy:  12.0\n",
      "Train Epoch: 9 [50/50000 (50%)]\tLoss: 2.070233\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 9 [60/50000 (60%)]\tLoss: 2.251699\n",
      "Train Accuracy:  15.714285714285714\n",
      "Train Epoch: 9 [70/50000 (70%)]\tLoss: 2.034142\n",
      "Train Accuracy:  18.75\n",
      "Train Epoch: 9 [80/50000 (80%)]\tLoss: 2.096712\n",
      "Train Accuracy:  18.88888888888889\n",
      "Train Epoch: 9 [90/50000 (90%)]\tLoss: 2.282395\n",
      "Train Accuracy:  19.0\n",
      "\n",
      "Test set: Average loss: 2.4258, Accuracy: 1055/10000 (10.55%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 2.019329\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 10 [10/50000 (10%)]\tLoss: 2.238501\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 10 [20/50000 (20%)]\tLoss: 2.621051\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 10 [30/50000 (30%)]\tLoss: 2.338204\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 10 [40/50000 (40%)]\tLoss: 2.398223\n",
      "Train Accuracy:  14.0\n",
      "Train Epoch: 10 [50/50000 (50%)]\tLoss: 2.042668\n",
      "Train Accuracy:  15.0\n",
      "Train Epoch: 10 [60/50000 (60%)]\tLoss: 2.235671\n",
      "Train Accuracy:  14.285714285714286\n",
      "Train Epoch: 10 [70/50000 (70%)]\tLoss: 2.084211\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 10 [80/50000 (80%)]\tLoss: 2.236225\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 10 [90/50000 (90%)]\tLoss: 2.044480\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.3652, Accuracy: 1296/10000 (12.96%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 2.168975\n",
      "Train Accuracy:  0.0\n",
      "Train Epoch: 11 [10/50000 (10%)]\tLoss: 2.269235\n",
      "Train Accuracy:  5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [20/50000 (20%)]\tLoss: 2.273019\n",
      "Train Accuracy:  3.3333333333333335\n",
      "Train Epoch: 11 [30/50000 (30%)]\tLoss: 2.022736\n",
      "Train Accuracy:  12.5\n",
      "Train Epoch: 11 [40/50000 (40%)]\tLoss: 2.354164\n",
      "Train Accuracy:  16.0\n",
      "Train Epoch: 11 [50/50000 (50%)]\tLoss: 2.175894\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 11 [60/50000 (60%)]\tLoss: 2.250979\n",
      "Train Accuracy:  17.142857142857142\n",
      "Train Epoch: 11 [70/50000 (70%)]\tLoss: 2.121816\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 11 [80/50000 (80%)]\tLoss: 2.278606\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 11 [90/50000 (90%)]\tLoss: 2.432130\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.4214, Accuracy: 1119/10000 (11.19%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.987909\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 12 [10/50000 (10%)]\tLoss: 2.242917\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 12 [20/50000 (20%)]\tLoss: 2.474226\n",
      "Train Accuracy:  6.666666666666667\n",
      "Train Epoch: 12 [30/50000 (30%)]\tLoss: 2.088539\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 12 [40/50000 (40%)]\tLoss: 2.202610\n",
      "Train Accuracy:  12.0\n",
      "Train Epoch: 12 [50/50000 (50%)]\tLoss: 2.451319\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 12 [60/50000 (60%)]\tLoss: 2.128919\n",
      "Train Accuracy:  17.142857142857142\n",
      "Train Epoch: 12 [70/50000 (70%)]\tLoss: 1.959390\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 12 [80/50000 (80%)]\tLoss: 2.310390\n",
      "Train Accuracy:  17.77777777777778\n",
      "Train Epoch: 12 [90/50000 (90%)]\tLoss: 2.218714\n",
      "Train Accuracy:  17.0\n",
      "\n",
      "Test set: Average loss: 2.4364, Accuracy: 1161/10000 (11.61%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 2.062920\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 13 [10/50000 (10%)]\tLoss: 2.113880\n",
      "Train Accuracy:  35.0\n",
      "Train Epoch: 13 [20/50000 (20%)]\tLoss: 2.359996\n",
      "Train Accuracy:  26.666666666666668\n",
      "Train Epoch: 13 [30/50000 (30%)]\tLoss: 2.360079\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 13 [40/50000 (40%)]\tLoss: 2.056136\n",
      "Train Accuracy:  22.0\n",
      "Train Epoch: 13 [50/50000 (50%)]\tLoss: 1.993617\n",
      "Train Accuracy:  28.333333333333332\n",
      "Train Epoch: 13 [60/50000 (60%)]\tLoss: 2.156914\n",
      "Train Accuracy:  27.142857142857142\n",
      "Train Epoch: 13 [70/50000 (70%)]\tLoss: 2.181071\n",
      "Train Accuracy:  26.25\n",
      "Train Epoch: 13 [80/50000 (80%)]\tLoss: 2.304883\n",
      "Train Accuracy:  24.444444444444443\n",
      "Train Epoch: 13 [90/50000 (90%)]\tLoss: 2.351691\n",
      "Train Accuracy:  23.0\n",
      "\n",
      "Test set: Average loss: 2.4455, Accuracy: 1097/10000 (10.97%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 2.106001\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 14 [10/50000 (10%)]\tLoss: 2.011685\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 14 [20/50000 (20%)]\tLoss: 2.292313\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 14 [30/50000 (30%)]\tLoss: 2.270829\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 14 [40/50000 (40%)]\tLoss: 2.307793\n",
      "Train Accuracy:  16.0\n",
      "Train Epoch: 14 [50/50000 (50%)]\tLoss: 2.252434\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 14 [60/50000 (60%)]\tLoss: 2.252533\n",
      "Train Accuracy:  11.428571428571429\n",
      "Train Epoch: 14 [70/50000 (70%)]\tLoss: 2.185743\n",
      "Train Accuracy:  12.5\n",
      "Train Epoch: 14 [80/50000 (80%)]\tLoss: 2.266889\n",
      "Train Accuracy:  13.333333333333334\n",
      "Train Epoch: 14 [90/50000 (90%)]\tLoss: 2.166186\n",
      "Train Accuracy:  16.0\n",
      "\n",
      "Test set: Average loss: 2.4347, Accuracy: 1088/10000 (10.88%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.915617\n",
      "Train Accuracy:  40.0\n",
      "Train Epoch: 15 [10/50000 (10%)]\tLoss: 2.309739\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 15 [20/50000 (20%)]\tLoss: 2.305779\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 15 [30/50000 (30%)]\tLoss: 2.033531\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 15 [40/50000 (40%)]\tLoss: 2.156119\n",
      "Train Accuracy:  28.0\n",
      "Train Epoch: 15 [50/50000 (50%)]\tLoss: 2.032025\n",
      "Train Accuracy:  28.333333333333332\n",
      "Train Epoch: 15 [60/50000 (60%)]\tLoss: 2.193625\n",
      "Train Accuracy:  25.714285714285715\n",
      "Train Epoch: 15 [70/50000 (70%)]\tLoss: 2.130878\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 15 [80/50000 (80%)]\tLoss: 2.138662\n",
      "Train Accuracy:  22.22222222222222\n",
      "Train Epoch: 15 [90/50000 (90%)]\tLoss: 2.341205\n",
      "Train Accuracy:  23.0\n",
      "\n",
      "Test set: Average loss: 2.4643, Accuracy: 1202/10000 (12.02%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.965761\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 16 [10/50000 (10%)]\tLoss: 2.136514\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 16 [20/50000 (20%)]\tLoss: 1.883279\n",
      "Train Accuracy:  26.666666666666668\n",
      "Train Epoch: 16 [30/50000 (30%)]\tLoss: 2.372950\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 16 [40/50000 (40%)]\tLoss: 2.164031\n",
      "Train Accuracy:  18.0\n",
      "Train Epoch: 16 [50/50000 (50%)]\tLoss: 2.286518\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 16 [60/50000 (60%)]\tLoss: 2.181947\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 16 [70/50000 (70%)]\tLoss: 2.202585\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 16 [80/50000 (80%)]\tLoss: 2.005984\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 16 [90/50000 (90%)]\tLoss: 2.502051\n",
      "Train Accuracy:  19.0\n",
      "\n",
      "Test set: Average loss: 2.4727, Accuracy: 1222/10000 (12.22%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 2.562373\n",
      "Train Accuracy:  10.0\n",
      "Train Epoch: 17 [10/50000 (10%)]\tLoss: 2.069405\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 17 [20/50000 (20%)]\tLoss: 2.218739\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 17 [30/50000 (30%)]\tLoss: 2.052787\n",
      "Train Accuracy:  22.5\n",
      "Train Epoch: 17 [40/50000 (40%)]\tLoss: 2.007317\n",
      "Train Accuracy:  24.0\n",
      "Train Epoch: 17 [50/50000 (50%)]\tLoss: 2.108583\n",
      "Train Accuracy:  23.333333333333332\n",
      "Train Epoch: 17 [60/50000 (60%)]\tLoss: 2.205722\n",
      "Train Accuracy:  22.857142857142858\n",
      "Train Epoch: 17 [70/50000 (70%)]\tLoss: 2.148222\n",
      "Train Accuracy:  21.25\n",
      "Train Epoch: 17 [80/50000 (80%)]\tLoss: 2.152317\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 17 [90/50000 (90%)]\tLoss: 2.362298\n",
      "Train Accuracy:  19.0\n",
      "\n",
      "Test set: Average loss: 2.4107, Accuracy: 1218/10000 (12.18%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 2.309503\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 18 [10/50000 (10%)]\tLoss: 2.154275\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 18 [20/50000 (20%)]\tLoss: 1.955748\n",
      "Train Accuracy:  23.333333333333332\n",
      "Train Epoch: 18 [30/50000 (30%)]\tLoss: 1.892728\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 18 [40/50000 (40%)]\tLoss: 2.327727\n",
      "Train Accuracy:  24.0\n",
      "Train Epoch: 18 [50/50000 (50%)]\tLoss: 2.704522\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 18 [60/50000 (60%)]\tLoss: 2.237121\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 18 [70/50000 (70%)]\tLoss: 2.091419\n",
      "Train Accuracy:  17.5\n",
      "Train Epoch: 18 [80/50000 (80%)]\tLoss: 2.172623\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 18 [90/50000 (90%)]\tLoss: 1.885172\n",
      "Train Accuracy:  21.0\n",
      "\n",
      "Test set: Average loss: 2.3859, Accuracy: 1323/10000 (13.23%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 2.142814\n",
      "Train Accuracy:  30.0\n",
      "Train Epoch: 19 [10/50000 (10%)]\tLoss: 2.014253\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 19 [20/50000 (20%)]\tLoss: 2.319867\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 19 [30/50000 (30%)]\tLoss: 2.428000\n",
      "Train Accuracy:  17.5\n",
      "Train Epoch: 19 [40/50000 (40%)]\tLoss: 2.308307\n",
      "Train Accuracy:  18.0\n",
      "Train Epoch: 19 [50/50000 (50%)]\tLoss: 2.135759\n",
      "Train Accuracy:  18.333333333333332\n",
      "Train Epoch: 19 [60/50000 (60%)]\tLoss: 2.259116\n",
      "Train Accuracy:  15.714285714285714\n",
      "Train Epoch: 19 [70/50000 (70%)]\tLoss: 2.076634\n",
      "Train Accuracy:  16.25\n",
      "Train Epoch: 19 [80/50000 (80%)]\tLoss: 2.178403\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 19 [90/50000 (90%)]\tLoss: 2.111511\n",
      "Train Accuracy:  18.0\n",
      "\n",
      "Test set: Average loss: 2.4185, Accuracy: 1185/10000 (11.85%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 2.017350\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 20 [10/50000 (10%)]\tLoss: 2.110529\n",
      "Train Accuracy:  25.0\n",
      "Train Epoch: 20 [20/50000 (20%)]\tLoss: 2.222427\n",
      "Train Accuracy:  16.666666666666668\n",
      "Train Epoch: 20 [30/50000 (30%)]\tLoss: 2.151134\n",
      "Train Accuracy:  22.5\n",
      "Train Epoch: 20 [40/50000 (40%)]\tLoss: 1.931015\n",
      "Train Accuracy:  20.0\n",
      "Train Epoch: 20 [50/50000 (50%)]\tLoss: 1.973856\n",
      "Train Accuracy:  21.666666666666668\n",
      "Train Epoch: 20 [60/50000 (60%)]\tLoss: 2.195559\n",
      "Train Accuracy:  22.857142857142858\n",
      "Train Epoch: 20 [70/50000 (70%)]\tLoss: 2.627072\n",
      "Train Accuracy:  21.25\n",
      "Train Epoch: 20 [80/50000 (80%)]\tLoss: 2.381279\n",
      "Train Accuracy:  21.11111111111111\n",
      "Train Epoch: 20 [90/50000 (90%)]\tLoss: 2.546346\n",
      "Train Accuracy:  20.0\n",
      "\n",
      "Test set: Average loss: 2.4541, Accuracy: 1167/10000 (11.67%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from kymatio import Scattering2D\n",
    "import torch\n",
    "import argparse\n",
    "#import kymatio.datasets as scattering_datasets\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scattering2dResNet(nn.Module):\n",
    "    def __init__(self, in_channels,  k=2, n=4, num_classes=10):\n",
    "        super(Scattering2dResNet, self).__init__()\n",
    "        self.inplanes = 16 * k\n",
    "        self.ichannels = 16 * k\n",
    "        self.K = in_channels\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels, eps=1e-5, affine=False),\n",
    "            nn.Conv2d(in_channels, self.ichannels,\n",
    "                  kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.ichannels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = self._make_layer(BasicBlock, 32 * k, n)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 64 * k, n)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(2)\n",
    "        self.fc = nn.Linear(64 * k * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.K, 8, 8)\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(model, device, train_subset_loader, optimizer, epoch, scattering):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_subset_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(scattering(data))\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_subset_loader.dataset),\n",
    "                100. * batch_idx / len(train_subset_loader), loss.item()))\n",
    "        \n",
    "        print('Train Accuracy: ', (100*correct)/total)\n",
    "        \n",
    "        \n",
    "def test(model, device, testloader, scattering):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(scattering(data))\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train a simple Hybrid Resnet Scattering + CNN model on CIFAR.\n",
    "\n",
    "        scattering 1st order can also be set by the mode\n",
    "        Scattering features are normalized by batch normalization.\n",
    "        The model achieves around 88% testing accuracy after 10 epochs.\n",
    "\n",
    "        scatter 1st order +\n",
    "        scatter 2nd order + linear achieves 70.5% in 90 epochs\n",
    "\n",
    "        scatter + cnn achieves 88% in 15 epochs\n",
    "\n",
    "    \"\"\"\n",
    "    description='CIFAR scattering  + hybrid examples'\n",
    "    mode = 1 #scattering 1st or 2nd order\n",
    "    width = 2 #width factor for resnet\n",
    "\n",
    "    try_cuda=False\n",
    "    device = 'cuda' if torch.cuda.is_available() and try_cuda else 'cpu'\n",
    "\n",
    "    if mode == 1:\n",
    "        scattering = Scattering2D(J=2, shape=(32, 32), max_order=1)\n",
    "        K = 17*3\n",
    "    else:\n",
    "        scattering = Scattering2D(J=2, shape=(32, 32))\n",
    "        K = 81*3\n",
    "    if try_cuda:\n",
    "        scattering = scattering.to(device)\n",
    "\n",
    "    model = Scattering2dResNet(K, width).to(device)\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Data Transformation\n",
    "    transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "\n",
    "    #Define Valid Indices\n",
    "    subset_indices = list(range(100))\n",
    "\n",
    "    #Using Torch Subset DataLoader to Load the valid indices\n",
    "    train_subset_loader = torch.utils.data.DataLoader(trainset, batch_size=10, sampler=data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Optimizer\n",
    "    lr = 0.1\n",
    "    num_epoch = 20\n",
    "    \n",
    "    print('Total Train Images: ', len(train_subset_loader)*train_subset_loader.batch_size)\n",
    "    print('Total Test Images: ', len(testloader)*testloader.batch_size)\n",
    "    for epoch in range(0, num_epoch):\n",
    "        if epoch%20==0:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,\n",
    "                                        weight_decay=0.0005)\n",
    "            lr*=0.2\n",
    "\n",
    "        train(model, device, train_subset_loader, optimizer, epoch+1, scattering)\n",
    "        test(model, device, testloader, scattering)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   Scattering Transform + ResNet  | 20 | 20% | 11.67% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weakly supervised techniques permit to tackle the issue of labeled data. An introduction to those techniques can be found here: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html.\n",
    "\n",
    "__(Open) Question 10:__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not attempt this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 11:__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we go over various techniques to address the problem of small datasets. Although in practice, deep-learning methods are extremely data hungry because of their high number of trainable parameters. However in this task, we dive into some of the techniques which can boost the performance of deep neural networks on small and mini datasets. The considered dataset in this task is a subset of 100 samples from CIFAR-10. CIFAR-10 is a widely used benchmark in computer vision and deep learning.\n",
    "\n",
    "\n",
    "Initially we compare the performance of VGG19 and ResNet18 architectures on our dataset. I trained both the models for 20 epochs and obtained accuracy of 23% on the test dataset using ResNet whereas obtained an accuracy of 12.15% using the VGG19 architecture. This is certainly down to the design of ResNets, where there skip connections providing high speed highway for gradient flow and therefore improve performance.\n",
    "\n",
    "\n",
    "After this we had to use a pretrained model on ImageNet and finetune it on our dataset. For this I used ResNet50 Architecure and finetuned the last layer. Amazingly just for 5 epochs, the performance on the training dataset reached 100% whereas the performance on the test dataset was quite poor numerically being 11.86%. We conclude from this that finetune (transfer learning) is an extremely effective technique which can help in dealing with mini datasets. And depending on the amount of data we have, we can choose train last layer or the last few layers etc.\n",
    "\n",
    "The other task involving pretrained model was to use a pretrained DCGAN model. After struggling alot, I was able to find a pretrained discriminator (trained for 290 epochs) for the DCGAN model on CIFAR100 dataset (link given just below question 6). I used the discriminator to extract the vector representations for the images in our training and test samples. On these vector representations, I trained a supervised SVM model and obtained an accuracy of 10.09% on the test dataset, which is not very good. Certainly a better discriminator can increase the performance. Through this task we learnt that using GANs as feature extractors for images can be useful when we have small data problems.\n",
    "\n",
    "The next notion involved using data augmentation and transformation methods. I augmented the dataset with a mix of affine transformations and color jittering in the HSV color space. And then used the same VGG19 and ResNet18 for training. Interestingly the performance of VGG19 and ResNet18 dropped on the test dataset (till 20 epochs atleast). I believe that the size of images in CIFAR10 is $32\\times32$ and applying data augmentation must have caused distortion, which led to the loss of performance. I believe training the model for more number of epochs can be helpful.\n",
    "\n",
    "Finally we had to train an end to end pipeline involving scattering transform and a ResNet based architecture. The idea is that scattering transforms preserves most of the signal information needed for classification while\n",
    "substantially reducing the spatial resolution and total signal size. I trained this pipeline for 20 epochs and obtained a test accuracy of 11.67%. An advantage of using scattering transforms as a base for deep nets is that it delivers substantial  improvements in  the  inference  speed and training memory consumption compared to\n",
    "models trained directly on the input images.\n",
    "\n",
    "\n",
    "The following figure summarizes the obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = {\n",
    "    \"Scattering+ResNet\": [20, 0.20, 0.1167], \n",
    "    \"ResNet18\": [20,0.98, .23], \n",
    "    \"ResNet18-Augmented\": [20, 0.44, .1262], \n",
    "    \"VGG19\": [20, 0.24, .1262], \n",
    "    \"VGG19-Augmented\": [20, 0.20, .1261],  \n",
    "    \"DCGAN+SVM\": [290, 1.0, 0.1009],  \n",
    "    \"ResNet-50 Pretrained\": [5, 1.0, 0.1186]}\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i, (name, values) in enumerate(models.items()):\n",
    "    plt.scatter(i, values[-1], label=name)\n",
    "plt.legend()\n",
    "plt.title(\"Summary of Results : Training 100 Samples of CIFAR10 Dataset\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
